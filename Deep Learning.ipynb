{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🧠 Convolutional Neural Networks (CNNs) - Architecture\n",
    "\n",
    "---\n",
    "\n",
    "## 📐 Architecture Components\n",
    "\n",
    "### 1. 🧭 Input Layer\n",
    "- Accepts image data, e.g., shape **(28×28×3)** for RGB images.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. 🧲 Convolutional Layer\n",
    "- Applies filters (kernels) to scan the image and detect features like edges, textures, and shapes.\n",
    "- Each filter generates a **feature map**.\n",
    "\n",
    "**Mathematically:**\n",
    "\n",
    "$$(f * x)(i, j) = \\sum_m \\sum_n x(i + m, j + n) \\cdot f(m, n)$$\n",
    "\n",
    "Where:\n",
    "- \\( x \\) = input image\n",
    "- \\( f \\) = filter (kernel)\n",
    "- \\( * \\) = convolution\n",
    "- \\( (i, j) \\) = output location\n",
    "\n",
    "---\n",
    "\n",
    "### 3. ⚡ Activation Function (ReLU)\n",
    "- Applies non-linearity:\n",
    "\n",
    "  $$\\text{ReLU}(x) = \\max(0, x)$$\n",
    "\n",
    "- Helps learn complex patterns.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. 🔽 Pooling Layer (Subsampling)\n",
    "- Reduces spatial dimensions, improves efficiency.\n",
    "- Commonly uses **Max Pooling** (e.g., 2×2).\n",
    "- Helps achieve **translation invariance**.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. 🧮 Fully Connected (Dense) Layer\n",
    "- Flattens the feature maps into a vector.\n",
    "- Passes through one or more dense layers to make predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. 🎯 Output Layer\n",
    "- Outputs class probabilities using **Softmax**:\n",
    "\n",
    "  $$\\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}$$\n",
    "\n",
    "---\n",
    "\n",
    "## 📈 Why CNNs Work Well for Images\n",
    "\n",
    "- **Local connectivity**: Focus on small image patches.\n",
    "- **Weight sharing**: Filters reused across space → fewer parameters.\n",
    "- **Translation invariance**: Pooling helps recognize patterns in varied positions.\n",
    "- **Hierarchical learning**: Layers learn from edges → shapes → objects.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 Key Considerations When Writing CNN Code\n",
    "\n",
    "### 1. 📥 Input Data\n",
    "- ✅ Ensure consistent image size (e.g., 224×224×3).\n",
    "- ✅ Normalize pixel values ([0, 1] or standardize).\n",
    "- ✅ Use data augmentation for robustness.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. 🏗 Model Architecture\n",
    "- ✅ Use small filters (e.g., 3×3), padding='same'.\n",
    "- ✅ Add pooling and dropout layers.\n",
    "- ✅ Batch normalization to stabilize training.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. 🔀 Activation Functions\n",
    "- ✅ Use ReLU after each convolution.\n",
    "- ❗ Avoid Sigmoid/Tanh in hidden layers unless needed.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. 🧾 Output Layer\n",
    "- ✅ Use `softmax` for multi-class, `sigmoid` for binary.\n",
    "- ✅ Match loss function to activation:\n",
    "  - `categorical_crossentropy` for one-hot\n",
    "  - `sparse_categorical_crossentropy` for integer labels\n",
    "\n",
    "---\n",
    "\n",
    "### 5. ⚙️ Optimizer\n",
    "- ✅ Try Adam, SGD, or RMSprop.\n",
    "- ✅ Start with learning rate ~0.001.\n",
    "- ❗ Use learning rate schedulers or early stopping for long training.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. 🧪 Training Strategy\n",
    "- ✅ Track train and val accuracy/loss.\n",
    "- ✅ Use `EarlyStopping`, `ModelCheckpoint`.\n",
    "- ✅ Consider `KFold` or cross-validation for small datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. 🚀 Hardware Utilization\n",
    "- ✅ Use GPU for acceleration (e.g., via Colab or CUDA).\n",
    "- ✅ Use `ImageDataGenerator` or `tf.data` for large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. ⚖️ Model Complexity\n",
    "- ❗ Don't overbuild models for small datasets.\n",
    "- ✅ Start small → add layers as needed.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. 🔁 Reproducibility\n",
    "- ✅ Set seeds for random number generators.\n",
    "- ✅ Save models, use `TensorBoard` or logging tools (e.g., WandB).\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Real-World Applications\n",
    "\n",
    "- ✅ Image classification (Cats vs Dogs)\n",
    "- ✅ Object detection (YOLO, SSD)\n",
    "- ✅ Face recognition\n",
    "- ✅ Medical imaging (CT, MRI)\n",
    "- ✅ OCR and document analysis\n",
    "\n",
    "---\n",
    "\n",
    "Would you like a TensorFlow or PyTorch code example to go with this?\n"
   ],
   "id": "9acb07f9f8bf93b0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ## 📌 1. Imports and Setup\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import random\n",
    "import os\n",
    "\n",
    "# ## 📌 2. Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "\n",
    "# ## 📌 3. Data Preparation\n",
    "# For demonstration, use CIFAR-10 dataset (10 classes, RGB images of 32x32)\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# One-hot encode labels\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "train_generator = train_datagen.flow(x_train, y_train, batch_size=64)\n",
    "\n",
    "# ## 📌 4. CNN Model Architecture\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(32, 32, 3)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# ## 📌 5. Compilation\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# ## 📌 6. Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "    ModelCheckpoint('best_model.h5', save_best_only=True)\n",
    "]\n",
    "\n",
    "# ## 📌 7. Training\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=30,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# ## 📌 8. Evaluation\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# ## 📌 9. Summary\n",
    "model.summary()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🔁 Recurrent Neural Networks (RNNs)\n",
    "\n",
    "---\n",
    "\n",
    "## 📐 Architecture Components\n",
    "\n",
    "### 1. 🧭 Input Layer\n",
    "- Accepts sequential data (e.g., text, time-series).\n",
    "- Shape: **(batch_size, time_steps, features)**.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. 🔁 Recurrent Layer\n",
    "- Processes sequence **one time step at a time**, maintaining a hidden state.\n",
    "- Core idea: **memory of previous inputs** helps predict current/future values.\n",
    "\n",
    "**Mathematical Representation:**\n",
    "\n",
    "Let:\n",
    "- \\( x_t \\) = input at time t\n",
    "- \\( h_t \\) = hidden state at time t\n",
    "- \\( W \\), \\( U \\), \\( b \\) = learnable parameters\n",
    "\n",
    "$$\n",
    "h_t = \\tanh(Wx_t + Uh_{t-1} + b)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 3. ⛓ Variants of RNN\n",
    "\n",
    "- **Vanilla RNN** – simple, but suffers from vanishing gradients.\n",
    "- **LSTM (Long Short-Term Memory)** – solves long-term dependency problems with gates (input, forget, output).\n",
    "- **GRU (Gated Recurrent Unit)** – similar to LSTM, simpler, faster to train.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. 🧮 Fully Connected (Dense) Layer\n",
    "- Final hidden state is passed to a dense layer for classification or regression.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. 🎯 Output Layer\n",
    "- Uses `softmax` for classification, `sigmoid` or `linear` for regression.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Why RNNs Are Useful\n",
    "\n",
    "- Capture **temporal dependencies** and **context** in sequences.\n",
    "- Great for tasks where **order matters**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Applications\n",
    "\n",
    "- Natural Language Processing (NLP): sentiment analysis, language modeling.\n",
    "- Time Series Forecasting: stock prices, weather.\n",
    "- Speech Recognition.\n",
    "- Music Generation.\n",
    "- Anomaly Detection.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 Key Considerations When Writing RNN Code\n",
    "\n",
    "### 1. 📥 Input Data\n",
    "- ✅ Shape: (samples, time_steps, features)\n",
    "- ✅ Tokenize and pad sequences for text data.\n",
    "- ✅ Normalize time series inputs.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. 🔁 RNN Layer Choice\n",
    "- ✅ Use `SimpleRNN` for small, simple problems.\n",
    "- ✅ Use `LSTM` or `GRU` for long sequences or better performance.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. 🔀 Activation Functions\n",
    "- ✅ Use `tanh` and `sigmoid` in LSTM/GRU.\n",
    "- ✅ Use `relu` in dense layers if needed.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. 🧾 Output Layer\n",
    "- ✅ `softmax` for multi-class output.\n",
    "- ✅ `sigmoid` for binary classification.\n",
    "- ✅ `linear` for regression.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. ⚙️ Optimizer and Loss\n",
    "- ✅ `categorical_crossentropy` or `sparse_categorical_crossentropy` for classification.\n",
    "- ✅ `mean_squared_error` or `mean_absolute_error` for regression.\n",
    "- ✅ Use `Adam` or `RMSprop` optimizers.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. ⏱ Sequence Handling\n",
    "- ✅ Use `return_sequences=True` if stacking RNNs.\n",
    "- ✅ Use `return_state=True` if you want to keep states for prediction chaining.\n",
    "- ✅ Use masking for padded inputs.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. 🧪 Training Strategy\n",
    "- ✅ Shuffle data only if sequence order doesn't matter.\n",
    "- ✅ Use `EarlyStopping` and `ModelCheckpoint`.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. ⚙️ Regularization\n",
    "- ✅ Use `Dropout` or `recurrent_dropout`.\n",
    "- ✅ Clip gradients if exploding gradients are observed.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. 🧠 Model Complexity\n",
    "- ❗ Don't over-stack RNNs.\n",
    "- ✅ Start with 1–2 layers, tune from there.\n",
    "\n",
    "---\n",
    "\n",
    "### 10. 🔁 Reproducibility\n",
    "- ✅ Set random seeds.\n",
    "- ✅ Log training metrics (TensorBoard, WandB).\n",
    "- ✅ Save model weights.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Sample Use Cases\n",
    "\n",
    "- Sentiment Analysis on movie reviews.\n",
    "- Predict next word in a sentence.\n",
    "- Forecast sales or electricity usage.\n",
    "- Detect anomalies in system logs.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like a working TensorFlow code example for this RNN setup?\n"
   ],
   "id": "82e1e5d0bb635a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 1. Generate Dummy Sequential Data\n",
    "np.random.seed(42)\n",
    "time_steps = 10\n",
    "features = 5\n",
    "samples = 1000\n",
    "\n",
    "X = np.random.rand(samples, time_steps, features)\n",
    "y = np.random.randint(0, 2, size=(samples, 1))  # Binary classification\n",
    "\n",
    "# 2. Normalize Features\n",
    "scaler = MinMaxScaler()\n",
    "X = X.reshape(-1, features)\n",
    "X = scaler.fit_transform(X)\n",
    "X = X.reshape(samples, time_steps, features)\n",
    "\n",
    "# 3. Train/Test Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. Build RNN Model\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=False, input_shape=(time_steps, features), dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')  # Binary classification\n",
    "])\n",
    "\n",
    "# 5. Compile Model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 6. Callbacks\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True)\n",
    "\n",
    "# 7. Train the Model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop, checkpoint],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 8. Evaluate\n",
    "loss, accuracy = model.evaluate(X_val, y_val)\n",
    "print(f\"Validation Loss: {loss:.4f} - Accuracy: {accuracy:.4f}\")"
   ],
   "id": "df770ca4ddc9dccf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
