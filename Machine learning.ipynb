{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T07:18:33.226475Z",
     "start_time": "2025-05-03T07:18:32.692594Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Perceptron\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ],
   "id": "9dee7650651d32ee",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ðŸ“˜ Linear Regression\n",
    "\n",
    "- **Purpose**: Predicts a continuous output variable based on input features.\n",
    "- **Use Case**: Forecasting numerical values like prices or sales.\n",
    "- **Note**: While primarily used for regression tasks, we can apply it here for demonstration purposes.\n",
    "- **Common Parameters**:\n",
    "  - `fit_intercept`: Whether to calculate the intercept.\n",
    "  - `normalize`: Whether to normalize the input variables (deprecated).\n",
    "  - `copy_X`: Whether to copy the input X.\n",
    "  - `n_jobs`: Number of jobs to run in parallel."
   ],
   "id": "8a28adaeaff68841"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-03T07:50:58.903057Z",
     "start_time": "2025-05-03T07:50:58.770709Z"
    }
   },
   "source": [
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "y_pred_lr_class = y_pred_lr.round().astype(int)\n",
    "print(\"Linear Regression Accuracy:\", accuracy_score(y_test, y_pred_lr_class))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Accuracy: 1.0\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ðŸ“˜ Logistic Regression\n",
    "\n",
    "- **Purpose**: Used for classification tasks; predicts probability of classes.\n",
    "- **Use Case**: Email spam detection, disease prediction, etc.\n",
    "- **Note**: Suitable for classification tasks with clear decision boundaries.\n",
    "- **Common Parameters**:\n",
    "  - `penalty`: 'l2' (default)\n",
    "  - `C`: Regularization strength (1.0 by default)\n",
    "  - `solver`: 'lbfgs', good for multiclass\n",
    "  - `max_iter`: Convergence iterations\n",
    "  - `multi_class`: 'multinomial' for multiclass problems\n"
   ],
   "id": "2184d5532590b2cc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T07:33:26.273564Z",
     "start_time": "2025-05-03T07:33:26.235412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "log_model = LogisticRegression(penalty='l2',multi_class='multinomial', solver='lbfgs', max_iter=200)\n",
    "log_model.fit(X_train, y_train)\n",
    "y_pred_log = log_model.predict(X_test)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_log))\n"
   ],
   "id": "47585ec5def3ab22",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\2. DIP\\2. LAB\\pythonProject\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ðŸ“˜ Decision Tree\n",
    "\n",
    "- **Purpose**: Splits data into decisions using tree structures.\n",
    "- **Use Case**: Fraud detection, credit scoring, etc.\n",
    "- **Note**: Easy to interpret and visualize but prone to overfitting.\n",
    "- **Common Parameters**:\n",
    "  - `criterion`: 'gini' or 'entropy'\n",
    "     - Gini might create a simpler tree faster.\n",
    "     -Entropy might explore more balanced splits with finer discrimination.\n",
    "\n",
    "\n",
    "  - `max_depth`: Limits tree depth\n",
    "  - `min_samples_split`: Minimum samples to split"
   ],
   "id": "49438379ecc05ae6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T07:34:15.196187Z",
     "start_time": "2025-05-03T07:34:15.145339Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dt_model = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, y_pred_dt))"
   ],
   "id": "9b42bd427c32266b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 1.0\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ðŸ“˜ Random Forest\n",
    "\n",
    "- **Purpose**: Ensemble of Decision Trees to improve accuracy.\n",
    "- **Use Case**: Feature importance, classification.\n",
    "- **Note**: Reduces overfitting compared to individual decision trees.\n",
    "- **Common Parameters**:\n",
    "  - `n_estimators`: Number of trees\n",
    "  - `max_depth`: Max tree depth\n",
    "  - `bootstrap`: Whether sampling with replacement"
   ],
   "id": "c4872fab108b1fb2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T07:34:58.017043Z",
     "start_time": "2025-05-03T07:34:57.606995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))"
   ],
   "id": "75620261379f932d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 1.0\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ðŸ“˜ Support Vector Machine (SVM)\n",
    "\n",
    "- **Purpose**: SVMs are used for classification and regression. They find the optimal hyperplane that best separates classes.\n",
    "- **Use Case**: Text classification, image recognition, gene expression classification.\n",
    "\n",
    "#### ðŸ”§ Common Parameters:\n",
    "\n",
    "- `C`:\n",
    "  - Regularization parameter.\n",
    "  - Higher values â†’ less regularization â†’ model fits training data more.\n",
    "  - Lower values â†’ more regularization â†’ model generalizes better.\n",
    "\n",
    "- `kernel`:\n",
    "  - Specifies the kernel type for non-linear classification.\n",
    "  - Options:\n",
    "    - `'linear'`: Use for linearly separable data.\n",
    "    - `'rbf'` (default): Good for most cases, handles non-linear decision boundaries.\n",
    "    - `'poly'`: Polynomial kernel; useful for more complex boundaries.\n",
    "    - `'sigmoid'`: Similar to neural networks' activation functions.\n",
    "\n",
    "- `gamma`:\n",
    "  - Kernel coefficient for `'rbf'`, `'poly'`, and `'sigmoid'`.\n",
    "  - Controls the influence of a single training example.\n",
    "  - High value â†’ closer fit to training data (may overfit).\n",
    "  - Low value â†’ smoother decision boundary (may underfit).\n",
    "\n",
    "- `degree`:\n",
    "  - Degree of the polynomial kernel function (`kernel='poly'`).\n",
    "  - Ignored by other kernels.\n",
    "\n",
    "- `probability`:\n",
    "  - If `True`, enables probability estimates via cross-validation (slower).\n",
    "\n",
    "- `shrinking`:\n",
    "  - Whether to use the shrinking heuristic. Usually left as `True`.\n",
    "\n",
    "- `class_weight`:\n",
    "  - Handles class imbalance by assigning weights. `'balanced'` adjusts weights inversely proportional to class frequencies.\n",
    "\n",
    "- `max_iter`:\n",
    "  - Hard limit on iterations. `-1` means no limit.\n",
    "\n",
    "#### âœ… Recommended Settings:\n",
    "- For multiclass classification: use `SVC(kernel='rbf', C=1.0, gamma='scale')`\n",
    "- For linear classification on high-dimensional data (e.g. text): use `LinearSVC` instead of `SVC(kernel='linear')` for performance.\n"
   ],
   "id": "8aeb11bfefe7ffc6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T07:36:26.466911Z",
     "start_time": "2025-05-03T07:36:26.438767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "svm_model = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
    "svm_model.fit(X_train, y_train)\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "print(\"SVM Accuracy:\", accuracy_score(y_test, y_pred_svm))"
   ],
   "id": "d5b53879e4f51005",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 1.0\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ðŸ“˜ K-Nearest Neighbors (KNN)\n",
    "\n",
    "- **Purpose**: Classifies based on majority vote of nearest neighbors.\n",
    "- **Use Case**: Recommendation systems, handwriting recognition.\n",
    "- **Note**: Simple and intuitive but can be computationally intensive with large datasets.\n",
    "- **Common Parameters**:\n",
    "  - `n_neighbors`: Number of neighbors\n",
    "  - `weights`: 'uniform' or 'distance'"
   ],
   "id": "9bb109c5c09699d5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T07:37:18.569927Z",
     "start_time": "2025-05-03T07:37:18.510549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize and train the model\n",
    "knn_model = KNeighborsClassifier(n_neighbors=3)\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_knn = knn_model.predict(X_test)\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "print(f\"KNN Accuracy: {accuracy_knn:.2f}\")\n"
   ],
   "id": "d7e27cddaeefa01f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy: 1.00\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ðŸ“˜ Naive Bayes Classifier\n",
    "\n",
    "- **Purpose**: Applies Bayes' theorem with the assumption of feature independence.\n",
    "- **Use Case**: Particularly effective for text classification problems.\n",
    "- **Note**: Simple and fast, but the independence assumption may not always hold.\n"
   ],
   "id": "e047ecbe0000463e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T07:22:08.412031Z",
     "start_time": "2025-05-03T07:22:08.401043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize and train the model\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_nb = nb_model.predict(X_test)\n",
    "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
    "print(f\"Naive Bayes Accuracy: {accuracy_nb:.2f}\")\n",
    "\n"
   ],
   "id": "f96bc246a3bc469b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.98\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ðŸ“˜ Gradient Boosting\n",
    "\n",
    "- **Purpose**: Builds trees sequentially to reduce errors.\n",
    "- **Use Case**: Web search ranking, classification tasks.\n",
    "- **Note**: Can be sensitive to noisy data but often provides high predictive accuracy.\n",
    "- **Common Parameters**:\n",
    "  - `n_estimators`, `learning_rate`, `max_depth`\n"
   ],
   "id": "318bb99c16bff4e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T07:39:17.517745Z",
     "start_time": "2025-05-03T07:39:16.647550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb_model.fit(X_train, y_train)\n",
    "y_pred_gb = gb_model.predict(X_test)\n",
    "print(\"Gradient Boosting Accuracy:\", accuracy_score(y_test, y_pred_gb))"
   ],
   "id": "eecbb7dce51cc8ad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Accuracy: 1.0\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ðŸ“˜ AdaBoost Classifier\n",
    "\n",
    "- **Purpose**: Combines multiple weak classifiers to create a strong classifier.\n",
    "- **Use Case**: Improves the performance of weak learners.\n",
    "- **Note**: Sensitive to noisy data and outliers.\n",
    "- - **Common Parameters**:\n",
    "  - `n_estimators`, `learning_rate`\n"
   ],
   "id": "a8ba5a9ce9b08501"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T07:40:08.422430Z",
     "start_time": "2025-05-03T07:40:07.902239Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize and train the model\n",
    "ab_model = AdaBoostClassifier(n_estimators=100, learning_rate=1.0)\n",
    "ab_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_ab = ab_model.predict(X_test)\n",
    "accuracy_ab = accuracy_score(y_test, y_pred_ab)\n",
    "print(f\"AdaBoost Accuracy: {accuracy_ab:.2f}\")\n"
   ],
   "id": "10f5bbb142538403",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy: 1.00\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ðŸ“˜ Perceptron\n",
    "\n",
    "- **Purpose**: A simple neural network model for binary classification.\n",
    "- **Use Case**: Suitable for linearly separable data.\n",
    "- **Note**: Can be sensitive to feature scaling.\n"
   ],
   "id": "b974effed14465f7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T07:23:33.097213Z",
     "start_time": "2025-05-03T07:23:33.071907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize and train the model\n",
    "perceptron_model = Perceptron()\n",
    "perceptron_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_perceptron = perceptron_model.predict(X_test)\n",
    "accuracy_perceptron = accuracy_score(y_test, y_pred_perceptron)\n",
    "print(f\"Perceptron Accuracy: {accuracy_perceptron:.2f}\")\n"
   ],
   "id": "2e39aa9e2e5bfaa3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron Accuracy: 0.93\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ef6fc90ffe03654c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
